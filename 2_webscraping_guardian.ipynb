{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Webscraping News sites\n",
    "## Guardian News scraper\n",
    "https://www.theguardian.com/uk\n",
    "\n",
    "The following code block imports the necessary modules then requests the URL. Once this is received it displays the first 50 characters. \n",
    "\n",
    "Note the 'headers' variable. This replicates the identity that a web browser would send to avoid the site blocking our request thinking it was a bot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "import re\n",
    "import csv\n",
    "import os.path\n",
    "\n",
    "\n",
    "headers = {\n",
    "    'Accept-Encoding': 'gzip, deflate, sdch',\n",
    "    'Accept-Language': 'en-US,en;q=0.8',\n",
    "    'Upgrade-Insecure-Requests': '1',\n",
    "    'User-Agent': 'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/56.0.2924.87 Safari/537.36',\n",
    "    'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',\n",
    "    'Cache-Control': 'max-age=0',\n",
    "    'Connection': 'keep-alive',\n",
    "}\n",
    "\n",
    "# url to fetch\n",
    "url = \"https://www.theguardian.com/uk\"\n",
    "\n",
    "# Request\n",
    "req = requests.get(url, headers=headers)\n",
    "\n",
    "\n",
    "# Save the front page content\n",
    "frontpage = req.content\n",
    "\n",
    "print(frontpage[:50])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  BeautifulSoup\n",
    "We can use BeautifulSoup to find all the article titles and display a total count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feed the page into Beautiful Soup\n",
    "soup = BeautifulSoup(frontpage, 'html.parser')\n",
    "\n",
    "# Find news articles\n",
    "frontpage_news = soup.find_all('h3', class_='fc-item__title')\n",
    "\n",
    "# total no of articles\n",
    "total =  (len(frontpage_news)) \n",
    "\n",
    "print(total)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## Create a CSV file to hold the articles\n",
    " In the next codeblock we declare the name of our CSV\\* file. If it doesn't exists it will be created.\n",
    " \n",
    " If the file does exist the URLs are extracted and stored in a list so we have a reference of articles already downloaded. We can then prin out this list for reference.\n",
    " \n",
    "\\* CSV stands for Comma Separated Values and this file type can be viewd in MS Excel or a text editor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# declare csv file name\n",
    "csv_file = 'guardian.csv'\n",
    "\n",
    "# create empty list for news article links\n",
    "links = []\n",
    "\n",
    "# Check if csv already exists and if so store the news article links in a list\n",
    "if os.path.exists(csv_file):\n",
    "    with open(csv_file, 'r') as f:\n",
    "        csvreader = csv.reader(f, delimiter=\",\")\n",
    "        for row in csvreader:\n",
    "            links.append(row[2])\n",
    "            print(row[2])\n",
    "\n",
    "# Open the file ready for writing\n",
    "file = open(csv_file, \"a\")\n",
    "writer = csv.writer(file, quoting=csv.QUOTE_ALL)\n",
    "\n",
    "cnt = 0 # Set a counter\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get the article\n",
    "the next block of code does the main work. It:\n",
    "\n",
    "- Extracts the URL\n",
    "- Requests the contents of this page\n",
    "- Identifies the various bits of content, title, date, etc\n",
    "- Checks that the article doesn't exist in the CSV file, and if not, writes it to the file.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for n in np.arange(0, total):\n",
    "\n",
    "    # Getting the article link\n",
    "    link = frontpage_news[n].find('a')['href']\n",
    "\n",
    "    # Getting the title\n",
    "    title = frontpage_news[n].find('a').get_text()\n",
    "\n",
    "    # Retrieving the page\n",
    "    page = requests.get(link)\n",
    "\n",
    "    print('found link - ' + link)\n",
    "\n",
    "    # Get the all of the page content\n",
    "    page_content = page.content\n",
    "\n",
    "    # Parse the page content with Beautiful Soup\n",
    "    soup_article = BeautifulSoup(page_content, 'html.parser')\n",
    "    article_content = soup_article.find('div', class_='content__article-body from-content-api js-article__body')\n",
    "    \n",
    "\n",
    "    if article_content != None: # Check there is an article on the page\n",
    "\n",
    "        body = article_content.find_all('p')\n",
    "\n",
    "        pubdate = soup_article.find('time')['datetime']\n",
    "        # Unifying the paragraphs\n",
    "        list_paragraphs = []\n",
    "\n",
    "\n",
    "        for p in np.arange(0, len(body)):\n",
    "\n",
    "            paragraph = body[p].get_text()\n",
    "            list_paragraphs.append(paragraph)\n",
    "            final_article = \" \".join(list_paragraphs)\n",
    "\n",
    "            # Removing non-breaking spaces\n",
    "            txt = re.sub(\"\\\\xa0\", \"\", final_article)\n",
    "\n",
    "            title = title.replace('\\n', ' ').replace('\\r', '').lstrip()\n",
    "\n",
    "\n",
    "        if link not in links:\n",
    "            print('Retrieving artice -- ' + title)\n",
    "            cnt +=1\n",
    "            writer.writerow([pubdate, title, link, txt])\n",
    "\n",
    "print('Added ' + str(cnt) + ' news articles to ' + csv_file)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Viewing the CSV file\n",
    "\n",
    "The file 'guardian.csv' will now be available to view or download. Go back to the Noteable Home Page tab to see the file.\n",
    "\n",
    "Running the above code again will add any new articles to the CSV file."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
