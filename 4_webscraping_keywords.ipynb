{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using the Newspaper module with keywords\n",
    "\n",
    "This module makes it much easier to scrape newspapers. Rather than investigating the structure of the webpage, the Newspaper module is capable of understanding the structure of many news sites and doing the hard work for you.\n",
    "\n",
    "Sites I have tested this with:\n",
    "\n",
    "- https://cnn.com/\n",
    "- https://bbc.co.uk/\n",
    "- https://www.telegraph.co.uk/\n",
    "- https://www.theguardian.com/\n",
    "- https://indiatoday.in/\n",
    "\n",
    "**IMPORTANT** Before you begin you need to install the Newspaper module by running the followin code block. You only need to do this once for every Noteable session\n",
    "\n",
    "You can choose to select articles containing specific keywords. Just provide you keywords (one or more) in the 'my_keywords'. If you want to include all kewords comment this line out by placing a '#' sign at the begiining of the line.\n",
    "\n",
    "By default the code selects articles that include ••any•• of the keywords but if you want to search for articles that conatain **all** the kwyword:\n",
    "\n",
    "Find the following line of code:\n",
    "\n",
    "> if any(s in txt for s in my_keywords):\n",
    "  \n",
    "Change this to :\n",
    "\n",
    "> if all(s in txt for s in my_keywords):\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: newspaper3k in /opt/conda/lib/python3.7/site-packages (0.2.8)\n",
      "Requirement already satisfied: feedparser>=5.2.1 in /opt/conda/lib/python3.7/site-packages (from newspaper3k) (5.2.1)\n",
      "Requirement already satisfied: feedfinder2>=0.0.4 in /opt/conda/lib/python3.7/site-packages (from newspaper3k) (0.0.4)\n",
      "Requirement already satisfied: beautifulsoup4>=4.4.1 in /opt/conda/lib/python3.7/site-packages (from newspaper3k) (4.8.1)\n",
      "Requirement already satisfied: requests>=2.10.0 in /opt/conda/lib/python3.7/site-packages (from newspaper3k) (2.22.0)\n",
      "Requirement already satisfied: PyYAML>=3.11 in /opt/conda/lib/python3.7/site-packages (from newspaper3k) (5.1.2)\n",
      "Requirement already satisfied: Pillow>=3.3.0 in /opt/conda/lib/python3.7/site-packages (from newspaper3k) (6.2.0)\n",
      "Requirement already satisfied: tinysegmenter==0.3 in /opt/conda/lib/python3.7/site-packages (from newspaper3k) (0.3)\n",
      "Requirement already satisfied: cssselect>=0.9.2 in /opt/conda/lib/python3.7/site-packages (from newspaper3k) (1.1.0)\n",
      "Requirement already satisfied: tldextract>=2.0.1 in /opt/conda/lib/python3.7/site-packages (from newspaper3k) (2.2.2)\n",
      "Requirement already satisfied: lxml>=3.6.0 in /opt/conda/lib/python3.7/site-packages (from newspaper3k) (4.5.1)\n",
      "Requirement already satisfied: python-dateutil>=2.5.3 in /opt/conda/lib/python3.7/site-packages (from newspaper3k) (2.8.0)\n",
      "Requirement already satisfied: jieba3k>=0.35.1 in /opt/conda/lib/python3.7/site-packages (from newspaper3k) (0.35.1)\n",
      "Requirement already satisfied: nltk>=3.2.1 in /opt/conda/lib/python3.7/site-packages (from newspaper3k) (3.4.4)\n",
      "Requirement already satisfied: six in /opt/conda/lib/python3.7/site-packages (from feedfinder2>=0.0.4->newspaper3k) (1.12.0)\n",
      "Requirement already satisfied: soupsieve>=1.2 in /opt/conda/lib/python3.7/site-packages (from beautifulsoup4>=4.4.1->newspaper3k) (1.9.4)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /opt/conda/lib/python3.7/site-packages (from requests>=2.10.0->newspaper3k) (1.25.3)\n",
      "Requirement already satisfied: idna<2.9,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests>=2.10.0->newspaper3k) (2.8)\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /opt/conda/lib/python3.7/site-packages (from requests>=2.10.0->newspaper3k) (3.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.7/site-packages (from requests>=2.10.0->newspaper3k) (2019.9.11)\n",
      "Requirement already satisfied: requests-file>=1.4 in /opt/conda/lib/python3.7/site-packages (from tldextract>=2.0.1->newspaper3k) (1.5.1)\n",
      "Requirement already satisfied: setuptools in /opt/conda/lib/python3.7/site-packages (from tldextract>=2.0.1->newspaper3k) (41.0.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install newspaper3k"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scraping the news site\n",
    "Now we have installed the Newspapers module we can use it to scrape the site.\n",
    "\n",
    "The following code block by default will search the guardian site and write results to 'guardian.csv'\n",
    "\n",
    "To change this, enter a different URL in the 'news_source' variable and enter a different filename in the 'csv_file' variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import newspaper\n",
    "import csv\n",
    "import os\n",
    "from datetime import date\n",
    "\n",
    "my_keywords =  ['coronavirus','Trump']\n",
    "\n",
    "\n",
    "# Declare news source\n",
    "news_source = 'https://indiatoday.in'\n",
    "# news_source = 'https://www.theguardian.com/'\n",
    "# news_source = 'https://bbc.co.uk/'\n",
    "# news_source = 'https://www.telegraph.co.uk/'\n",
    "# news_source = 'https://cnn.com/'\n",
    "\n",
    "# declare export file name - news articles are written to this csv file\n",
    "csv_file = 'indiatoday.csv'\n",
    "\n",
    "paper = newspaper.build(news_source,  memoize_articles=False)\n",
    "\n",
    "# create empty list for existing news article links\n",
    "links = []\n",
    "\n",
    "# Check if csv already exists and if so store the news article links in a list\n",
    "if os.path.exists(csv_file):\n",
    "    with open(csv_file, 'r') as f:\n",
    "        csvreader = csv.reader(f, delimiter=\",\")\n",
    "        for row in csvreader:\n",
    "            links.append(row[2])\n",
    "            # print(row[2]) # show existing links\n",
    "\n",
    "\n",
    "# Open the file ready for writing\n",
    "file = open(csv_file, \"a\")\n",
    "writer = csv.writer(file, quoting=csv.QUOTE_ALL)\n",
    "\n",
    "cnt = 0 # Set a counter\n",
    "\n",
    "\n",
    "for article in paper.articles:\n",
    "    if article.url not in links:\n",
    "        # Retrieving the page\n",
    "        article.download()\n",
    "        article.parse()\n",
    "\n",
    "        # Getting the article link\n",
    "        link = article.url\n",
    "\n",
    "        # Getting the title\n",
    "        title = article.title\n",
    "\n",
    "        # Getting the authors\n",
    "        authors = article.authors\n",
    "        authors = ', '.join(authors) # convert authors to a comma separated list\n",
    "\n",
    "        # Get  all of the page content\n",
    "        txt = article.text\n",
    "\n",
    "        # Removing line-breaks\n",
    "        txt = txt.replace('\\n', ' ').replace('\\r', '')\n",
    "\n",
    "        # Get publication date\n",
    "        pubdate = article.publish_date\n",
    "\n",
    "        # Perform Natural Language Processing on text to extract keywords\n",
    "        article.nlp()\n",
    "        keywords = ', '.join(article.keywords) # convert keywords to a comma separated list\n",
    "\n",
    "        if txt != None:  # Check there is an article on the page\n",
    "            if 'my_keywords' in globals():\n",
    "                if any(s in txt for s in my_keywords): # check article for keywords (change any to all if required)\n",
    "                    print(\"Keyword/s found in article\")\n",
    "\n",
    "                    # Check if article exists already\n",
    "                    if link not in links:\n",
    "                        print('Retrieving article -- ' + title)\n",
    "                        cnt += 1\n",
    "                        writer.writerow([pubdate, title, link, authors, txt, keywords])\n",
    "\n",
    "                else:\n",
    "                    print ('Article doesn\\'t containing keword/s - skipping')\n",
    "            else:\n",
    "              # Check if article exists already\n",
    "              if link not in links:\n",
    "                  print('Retrieving article -- ' + title)\n",
    "                  cnt += 1\n",
    "                  writer.writerow([pubdate, title, link, authors, txt, keywords])\n",
    "\n",
    "print('Added ' + str(cnt) + ' news articles to ' + csv_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
